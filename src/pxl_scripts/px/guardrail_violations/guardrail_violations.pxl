# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

'''
This script shows LLM guardrail policy violations
'''
import px


def http_data(start_time: str, source_filter: str, destination_filter: str, num_head: int):

    df = px.DataFrame(table='http_events', start_time=start_time)
    min_time = df.agg(min_time=('time_', px.min))
    min_time_df = df.merge(min_time, how='inner', left_on='time_', right_on='min_time')
    # Add context.
    df.node = df.ctx['node']
    df.pid = px.upid_to_pid(df.upid)
    df = add_source_dest_columns(df)
    df = add_source_dest_links(df, start_time)

    # Filter out entities as specified by the user.
    df = df[px.contains(df.source, source_filter)]
    df = df[px.contains(df.destination, destination_filter)]

    # Restrict number of results.
    df = df.head(num_head)

    df = df[px.contains(df.resp_body, "The request did not pass the guardrails")]
    df.errors = px.pluck_array(px.pluck(px.pluck(px.pluck(df.resp_body, 'error'), 'details'), 'errors'), 0)
    df.violation = px.pluck(px.pluck(df.errors, 'guardrail'), 'content')
    df.violation = px.select(px.contains(df.violation, 'Escape the inner triple quotes'), px.substring(df.violation, 35, 1000), df.violation)
    df.model = px.pluck(df.req_body, 'model')
    df.count = px.select(df.violation == '', 0, 1)
    df.timestamp = px.bin(df.time_, window_ns)

    df = df.groupby(['destination', 'model', 'timestamp']).agg(
        throughput_total=('count', px.sum),
    )

    df['time_'] = df.timestamp
    return df


def add_source_dest_columns(df):
    ''' Add source and destination columns for the HTTP request.

    HTTP requests are traced server-side (trace_role==2), unless the server is
    outside of the cluster in which case the request is traced client-side (trace_role==1).

    When trace_role==2, the HTTP request source is the remote_addr column
    and destination is the pod column. When trace_role==1, the HTTP request
    source is the pod column and the destination is the remote_addr column.

    Input DataFrame must contain trace_role, upid, remote_addr columns.
    '''
    df.pod = df.ctx['pod']
    df.namespace = df.ctx['namespace']

    # If remote_addr is a pod, get its name. If not, use IP address.
    df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
    df.is_ra_pod = df.ra_pod != ''
    df.ra_name = px.select(df.is_ra_pod, df.ra_pod, df.remote_addr)

    df.is_server_tracing = df.trace_role == 2
    df.is_source_pod_type = px.select(df.is_server_tracing, df.is_ra_pod, True)
    df.is_dest_pod_type = px.select(df.is_server_tracing, True, df.is_ra_pod)

    # Set source and destination based on trace_role.
    df.source = px.select(df.is_server_tracing, df.ra_name, df.pod)
    df.destination = px.select(df.is_server_tracing, df.pod, df.ra_name)

    # Filter out messages with empty source / destination.
    df = df[df.source != '']
    df = df[df.destination != '']

    df = df.drop(['ra_pod', 'is_ra_pod', 'ra_name', 'is_server_tracing'])

    return df


def add_source_dest_links(df, start_time: str):
    ''' Modifies the source and destination columns to display deeplinks in the UI.
    Clicking on a pod name in either column will run the px/pod script for that pod.
    Clicking on an IP address, will run the px/ip script showing all network connections
    to/from that IP address.

    Input DataFrame must contain source, destination, is_source_pod_type,
    is_dest_pod_type, and namespace columns.
    '''

    # Source linking. If source is a pod, link to px/pod. If an IP addr, link to px/net_flow_graph.
    df.src_pod_link = px.script_reference(df.source, 'px/pod', {
        'start_time': start_time,
        'pod': df.source
    })
    df.src_link = px.script_reference(df.source, 'px/ip', {
        'start_time': start_time,
        'ip': df.source,
    })
    df.source = px.select(df.is_source_pod_type, df.src_pod_link, df.src_link)

    # If destination is a pod, link to px/pod. If an IP addr, link to px/net_flow_graph.
    df.dest_pod_link = px.script_reference(df.destination, 'px/pod', {
        'start_time': start_time,
        'pod': df.destination
    })
    df.dest_link = px.script_reference(df.destination, 'px/ip', {
        'start_time': start_time,
        'ip': df.destination,
    })
    df.destination = px.select(df.is_dest_pod_type, df.dest_pod_link, df.dest_link)

    df = df.drop(['src_pod_link', 'src_link', 'is_source_pod_type', 'dest_pod_link',
                  'dest_link', 'is_dest_pod_type'])

    return df

# ----------------------------------------------------------------
# Visualization Variables - No need to edit for basic configuration.
# ----------------------------------------------------------------
# K8s object is the abstraction to group on.
# Options are ['pod', 'service'].
k8s_object = 'deployment'
ns_per_ms = 1000 * 1000
ns_per_s = 1000 * ns_per_ms
# Window size to use on time_ column for bucketing.
window_ns = px.DurationNanos(10 * ns_per_s)
# Flag to filter out requests that come from an unresolvable IP.
filter_unresolved_inbound = True
# Flag to filter out health checks from the data.
filter_health_checks = True
# Flag to filter out ready checks from the data.
filter_ready_checks = True
# Flag to filter out non_k8s_traffic from the data.
filter_non_k8s_traffic = True
# The name of the incoming traffic column in the edge graphs.
src_col = 'requestor'
# The name of the outgoing traffic column in the edge graphs.
dest_col = 'responder'
# Column naem used to split data into separate time series.
# k8s_object column is renamed to this and is used in
# visualization spec.
split_series_name = 'k8s'
# The bin size to use for the latency histogram.
latency_bin_size_ns = px.DurationNanos(50 * ns_per_ms)
# ----------------------------------------------------------------


def svc_let(start_time: str):
    """ Calculates LET filtered on the svc

    Args:
    @start_time The timestamp of data to start at.
    @svc: the partial/full-name of the svc to
        calculate LET.

    Returns: DataFrame of the LET stats for svcs that
        match @svc.
    """
    exclude_in_cluster = False
    exclude_external = True
    df = make_http_table(start_time, exclude_in_cluster, exclude_external)
    # Calculate LET of svc(s) (k8s_object) over the time window ('timestamp')
    # after filtering for matching svcs.
    matching_df = df
    let_df = calc_model_http_LET(matching_df, [k8s_object, 'model', 'timestamp'])

    # Format and organize resulting columns.
    let_df[split_series_name] = let_df['model']
    let_df = let_df[['time_', split_series_name, 'model',
                     'latency_p50', 'latency_p90', 'latency_p99',
                     'ttft_latency_p50', 'ttft_latency_p90', 'ttft_latency_p99',
                     'itl_latency_p50', 'itl_latency_p90', 'itl_latency_p99',
                     'error_rate', 'request_throughput', 'token_throughput', 'bytes_throughput']]
    return let_df


def http_code_histogram(start_time: str, svc: px.Service):
    """ Computes a histogram of HTTP status codes

    Args:
    @start_time The timestamp of data to start at.
    @svc: the partial/full-name of the svc.

    Returns: DataFrame of the HTTP status code stats for svcs that
        match @svc.
    """
    df = make_http_table(start_time)
    matching_df = df[px.contains(df[k8s_object], svc)]
    return matching_df.groupby(['resp_status']).agg(count=('latency', px.count))


def latency_histogram(start_time: str, svc: px.Service):
    """ Computes a histogram of HTTP request latency.

    Args:
    @start_time The timestamp of data to start at.
    @svc: the partial/full-name of the svc.

    Returns: DataFrame of the HTTP latency histogram for svcs that
        match @svc.
    """
    df = make_http_table(start_time)
    matching_df = df[px.contains(df[k8s_object], svc)]
    matching_df.request_latency = px.DurationNanos(px.bin(matching_df.latency,
                                                          latency_bin_size_ns))
    return matching_df.groupby('request_latency').agg(count=('resp_status', px.count))


def outgoing_edges(start_time: str, exclude_in_cluster: bool, exclude_external: bool):
    """ Determines the svcs that this svc makes
    requests to.

    Args:
    @start_time The timestamp of data to start at.
    @svc: the partial/full-name of the svc to
        get outgoing_edges.

    Returns: DataFrame of the svcs this svc talks to
        and the LET summary of that communication.
    """
    df = let_per_edge(start_time, exclude_in_cluster, exclude_external)
    # TODO(ddelnano): Later filter for traffic that starts at this svc.
    outgoing_let_df = df
    # Group outgoing traffic by (src, dest) to get all outgoing edges.
    return summarize_LET(outgoing_let_df, [src_col, dest_col])


def incoming_edges(start_time: str, exclude_in_cluster: bool, exclude_external: bool):
    """ Determines the svcs that make requests to this svc.

    Args:
    @start_time The timestamp of data to start at.
    @svc: the partial/full-name of the svc to
        get incoming_edges.

    Returns: DataFrame of the svcs that talk to this svc
        and the LET summary of that communication.
    """
    df = let_per_edge(start_time, exclude_in_cluster, exclude_external)

    # TODO(ddelnano): Add filtering for traffic that ends at this svc.
    incoming_let_df = df

    # Group incoming traffic by (src, dest) to get all incoming edges.
    return summarize_LET(incoming_let_df, [src_col, dest_col])


def make_gpu_process_stats(start_time: str):
    gpu = px.DataFrame('gpu_process_stats', start_time=start_time)
    gpu.device_id = "0"
    gpu[src_col] = gpu.ctx['service']
    gpu[dest_col] = 'host: ' + gpu.ctx['node'] + '\ndevice id: ' + gpu.device_id
    gpu.error_rate = 0.0
    gpu.bytes_throughput = 1000.0
    gpu.request_throughput = 1000.0
    return gpu[[src_col, dest_col, 'error_rate', 'bytes_throughput', 'request_throughput']]


def svc_graph(start_time: str, svc: px.Service, exclude_in_cluster: bool, exclude_external: bool):
    """ Determines the svcs that make requests to this svc.
    Like incoming_edges, but filters out empty strings in the src/dest.

    Args:
    @start_time The timestamp of data to start at.
    @svc: the partial/full-name of the svc to
        get svc_graph in.

    Returns: DataFrame of the svcs that talk to this svc
        and the LET summary of that communication.
    """

    df = incoming_edges(start_time, exclude_in_cluster, exclude_external)
    outgoing = outgoing_edges(start_time, exclude_in_cluster, exclude_external)
    gpu = make_gpu_process_stats(start_time).append(outgoing)
    df = df.append(gpu)
    return df[df[src_col] != '' and df[dest_col] != '']


# ----------------------------------------------------------------
# Utility functions:
#
# These are shared functions. We plan to support imports in v0.3,
# which will allow these functions to be shared across multiple
# scripts.
# ----------------------------------------------------------------
def make_http_table(start_time: str, exclude_in_cluster: bool, exclude_external: bool):
    """ Makes the HTTP table given the passed in start.

    The data necessary to compute HTTP level svc information is located in the
    http_events table. We filter and aggregate data from this table to compute the
    required metrics.

    Args:
    @start_time The timestamp of data to start at.

    Returns: DataFrame of HTTP events with formatted columns.
    """
    df = px.DataFrame(table='http_events', start_time=start_time)
    df = format_http_table(df, filter_health_checks,
                           filter_ready_checks, filter_unresolved_inbound, exclude_in_cluster, exclude_external)
    return df


def let_per_edge(start_time: str, exclude_in_cluster: bool, exclude_external: bool):
    """ Calculates the LET per edge of the svc graph.

    Args:
    @start_time The timestamp of data to start at.

    Returns: DataFrame of HTTP events with formatted columns.
    """

    df = make_http_table(start_time, exclude_in_cluster, exclude_external)
    # Calculate LET for each svc edge in the svc graph over each time window.
    # Each edge starts at a requester ('remote_addr') and ends at a
    # responder (k8s_object).
    hostname_tracker = px.DataFrame('hostname_tracker', start_time=start_time).drop(['time_'])

    edge_let_df = calc_model_http_LET(df, ['remote_addr', k8s_object, 'timestamp'])
    # Convert 'remote_addr' IP into a svc name.
    edge_let_df = ip_to_svc_name(edge_let_df, 'remote_addr', src_col)
    # Rename k8s_object to dest_col.
    edge_let_df[dest_col] = edge_let_df[k8s_object]
    edge_let_df = edge_let_df.merge(hostname_tracker, how='left', left_on='remote_addr', right_on='ip_addr')
    edge_let_df[src_col] = px.select(edge_let_df.hostname != "", edge_let_df.hostname, edge_let_df[src_col])
    return edge_let_df


def format_events_table(df, latency_col, exclude_in_cluster, exclude_external):
    """ Format data and add semantic columns in event tables

    Unifies latency column to 'latency', adds a binned
    timestamp field to aggregate on, and adds the svc
    (k8s_object) as a semantic column.

    Works on "mysql_events" and "http_events"

    Args:
    @df: the input events table
    @latency_col: the name of the latency column in @df.

    Returns: formatted events DataFrame
    """
    df.latency = df[latency_col]

    df.timestamp = px.bin(df.time_, window_ns)
    df.model = px.pluck(df.req_body, 'model')
    df[k8s_object] = df.ctx[k8s_object]
    df.is_server_side_tracing = df.trace_role == 2
    df.match_traffic_type = px.select(df.is_server_side_tracing, not exclude_in_cluster, not exclude_external)
    df = df[(df[k8s_object] != '' and df.model != '') and df.match_traffic_type]
    df.exclude_external = exclude_external
    df.exclude_in_cluster = exclude_in_cluster

    df.model = df.model + ' ' + df[k8s_object]
    df.tokens = px.atoi(px.replace('.*"(?:completion_tokens|eval_count)":(\d+),.*', df.resp_body, '\\1'), 0)

    return df


def format_http_table(df, filter_health_checks, filter_ready_checks,
                      filter_unresolved_inbound, exclude_in_cluster, exclude_external):
    """ Formats HTTP events tables

    Runs events table universal formatting, adds a response_size,
    creates a failure field marking which requests receive an error
    status code, and optionally filters out system monitoring requests
    and partial data points.

    Args:
    @df: the input http_events table.
    @filter_health_checks: flag to filter health checks.
    @filter_ready_checks: flag to filter health checks.
    @filter_unresolved_inbound: flag to filter unresolved inbound
        requests.

    Returns: formatted HTTP events DataFrame.
    """
    df = format_events_table(df, 'latency', exclude_in_cluster, exclude_external)
    df.failure = df.resp_status >= 400
    df.is_healthcheck = px.contains(df.req_headers, "kube-probe") or df.req_path == '/healthz'
    filter_out_conds = (
        (not df.is_healthcheck or not filter_health_checks) and
         df.req_path != '/readyz' or not filter_ready_checks) and (
         df['remote_addr'] != '-' or not filter_unresolved_inbound)

    df = df[filter_out_conds]
    return df


def format_LET_aggs(df):
    """ Converts the result of LET windowed aggregates into expected metrics.

    Converts the result of aggregates on windows into well-formatted metrics that
    can be visualized. Latency quantile values need to be extracted from the
    quantiles struct, and then error_rate, request_throughput, and bytes_throughput
    are calculated as a function of window size.


    This function represents logic shared by LET calculators for MySQL and
    HTTP events.

    Args:
    @df: the input events table grouped into windows with aggregated
        columns 'throughput_total', 'error_rate_per_window', and 'request_throughput'

    Returns: DataFrame with formatted LET metrics.
    """
    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p99')))
    df.ttft_latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.ttft_quantiles, 'p50')))
    df.ttft_latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.ttft_quantiles, 'p90')))
    df.ttft_latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.ttft_quantiles, 'p99')))
    df.itl_latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.itl_quantiles, 'p50')))
    df.itl_latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.itl_quantiles, 'p90')))
    df.itl_latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.itl_quantiles, 'p99')))
    df['time_'] = df['timestamp']
    df.request_throughput = df.throughput_total / window_ns
    df.token_throughput = df.token_total / window_ns
    df.bytes_throughput = df.bytes_total / window_ns
    df.error_rate = df.error_rate_per_window * df.request_throughput / px.DurationNanos(1)

    return df


def calc_model_http_LET(df, groups):
    """ Calculates Latency, Error Rate, and Throughput on HTTP events.

    Calculates latency, error rate, and throughput aggregated over
    @groups. Throughput is represented by two values: request_throughput, and bytes_throughput.

    Args:
    @df: the input http_events table.
    @groups: the list of columns to group on. 'timestamp' must be a a group
        or this will fail.

    Returns: The LET DataFrame.
    """
    df = df[df.tokens > 1]
    df.itl = (df.latency - df.ttfb) / (df.tokens - 1)
    # Aggregate values over the window.
    df = df.groupby(groups).agg(
        latency_quantiles=('latency', px.quantiles),
        ttft_quantiles=('ttfb', px.quantiles),
        itl_quantiles=('itl', px.quantiles)
        error_rate_per_window=('failure', px.mean),
        throughput_total=('latency', px.count),
        token_total=('tokens', px.sum),
        bytes_total=('resp_body_size', px.sum)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df = format_LET_aggs(df)
    return df


def ip_to_svc_name(df, ip_col, svc_col_name):
    """ Map IP to svc name.

    Maps IP values stored in @ip_col into svc names to store into
    @svc_col_name.

    Args:
    @df: the input dataframe.
    @ip_col: the IP column to map from.
    @svc_col_name: the column name to assign the new svc values.

    Returns: DataFrame with the svc_col added.
    """
    pod_id = 'pod_id'
    df[pod_id] = px.ip_to_pod_id(df[ip_col])
    df[svc_col_name] = px.pod_id_to_service_name(df[pod_id])
    return df.drop(pod_id)


def summarize_LET(let_df, groups):
    """ Aggregate LET values across all windows.

    Args:
    @let_df: the DataFrame with LET values.
    @groups: the columns to group over.

    Returns: The summary DF.
    """

    df = let_df.groupby(groups).agg(
        request_throughput=('request_throughput', px.mean),
        bytes_throughput=('bytes_throughput', px.mean),
        error_rate=('error_rate', px.mean),
    )
    return df

